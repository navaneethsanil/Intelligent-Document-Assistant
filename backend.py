import faiss

from pathlib import Path
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain_core.prompts import PromptTemplate
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_community.vectorstores import FAISS


def initialize_llm(api_key: str):
    """
    Initializes the OpenAI LLM (gpt-4o) with specified parameters.

    Args:
        api_key (str): OpenAI API key.

    Returns:
        ChatOpenAI: An instance of the OpenAI chat model.
    """
    llm = ChatOpenAI(
        model="gpt-4o",
        temperature=0,
        api_key=api_key
    )
    return llm


def upload_document(doc_path: str, api_key: str):
    """
    Loads a document, splits it into chunks, embeds it, and stores it in a FAISS vector index.

    Args:
        doc_path (str): Path to the document (.pdf or .docx).
        api_key (str): OpenAI API key.

    Raises:
        ValueError: If the document type is not supported.
    """
    # Initialize the embedding model
    embedding_model = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=api_key)
    
    # Create a new FAISS index
    index = faiss.IndexFlatL2(len(embedding_model.embed_query("hello world")))
    vector_store = FAISS(
        embedding_function=embedding_model,
        index=index,
        docstore=InMemoryDocstore(),
        index_to_docstore_id={},
    )

    # Load document using the appropriate loader
    if doc_path.endswith(".pdf"):
        loader = PyPDFLoader(doc_path)
    elif doc_path.endswith(".docx"):
        loader = Docx2txtLoader(doc_path)
    else:
        raise ValueError("Unsupported file type")

    # Load and split the document into smaller chunks
    doc = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_documents(doc)

    # Embed and store the chunks
    vector_store.add_documents(chunks)
    vector_store.save_local("document-index")


# Prompt template used by the LLM
prompt = PromptTemplate.from_template("""
You are an intelligent AI assistant, designed to respond solely based on the information provided within the context below:
<context>
{context}
</context>

User Question: {input}                                 
""")


def query_document(api_key: str, question: str):
    """
    Loads the saved FAISS index and runs a retrieval-augmented query over it.

    Args:
        api_key (str): OpenAI API key.
        question (str): User's question.

    Returns:
        str: The answer generated by the LLM.
    """
    llm = initialize_llm(api_key=api_key)

    # Ensure index directory exists
    file_path = Path("document-index/index.faiss")
    file_path.parent.mkdir(parents=True, exist_ok=True)

    # Load vector store from local storage
    vector_store = FAISS.load_local(
        "document-index", 
        OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=api_key),
        allow_dangerous_deserialization=True
    )

    # Create document QA chain
    document_chain = create_stuff_documents_chain(llm, prompt)
    retriever = vector_store.as_retriever()
    retrieval_chain = create_retrieval_chain(retriever, document_chain)

    # Query the document
    response = retrieval_chain.invoke({"input": question})

    return response["answer"] if isinstance(response, dict) else str(response)
